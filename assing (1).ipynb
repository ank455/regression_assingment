{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2c452-5d81-4f6e-9f89-5dc44a941fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1\n",
    "\n",
    "Simple linear regression has only one x and one y variable.\n",
    "example - when we predict rent based on square feet alone that is simple linear regression.\n",
    "\n",
    "Multiple linear regressionn has only one y and more than two x varialbes .\n",
    "example - When we predict rent based on square feet and age of the building that is an example of multiple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f2d957-2ea4-4473-bb61-ad9cfb44c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 2\n",
    " There are five assumption of linear regression-\n",
    "    \n",
    "Assumption One: Linearity of the Data \n",
    "There is a linear relationship between the predictors(x) and the outcome\n",
    "for check - Linearity of the relationship \\t\\t\n",
    "\n",
    "Assumption Two: Predictors (x) are Independent & Observed with Negligible Error \n",
    "Predictor(x) are independent and observed with negligible error \n",
    "for check - Independence of errors \n",
    "\n",
    "Assumption Three: Residual Errors have a Mean Value of Zero\n",
    "Residual errors have a mean value zero\n",
    "for check - Homoscedasticity of errors\n",
    "\n",
    "Assumption Four: Residual Errors have Constant Variance\n",
    "Residual Errors have constant variance \n",
    "for check - Normality of errors\n",
    "\n",
    "\n",
    "Assumption Five: Residual Errors are Independent from Each Other & Predictors (x)\n",
    "Residual errors are independent from each other and predictors(x)\n",
    "for check - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbfaa6c-8594-446d-9a77-cc2e9eda4098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3\n",
    "\n",
    "Interpret slope and intercept in linear models is to first understand the slope-intercept formula: y = mx + b. M is the slope or the consistent change between x and y, and b is the y-intercept. Often, the y-intercept represents the starting point of the equation.\n",
    "    \n",
    "Example - If the speed of the club hitting the ball increases by 1 mph, then the model predicts that the length the ball travels increases by 57.66 yards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d901b510-4307-4efc-b34d-5f1ee466d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 4\n",
    "Gradient Descent is known as one of the most commonly used optimization algorithms to train machine learning models by means of minimizing errors between actual and expected results. Further, gradient descent is also used to train Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91e360d-8979-49ca-9ebd-396c7d159577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 5\n",
    "Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions with multiple explanatory variables. Whereas linear regress only has one independent variable impacting the slope of the relationship, multiple regression incorporates multiple independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79224a-ea76-4299-b5fd-d41521f8f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 6\n",
    "Multicollinearity occurs when two or more independent variables in a data frame have a high correlation with one another in a regression model.\n",
    "\n",
    "formula vif = 1/(1 - R**2) or \" from statsmodels.stats.outliers_influence import variance_inflation_factor\".\n",
    "\n",
    "If the vif score is greater than 5, it indicates presence of multicollinearity. Then you need feature transformation techniques like log transformation(works for most of the time). If you cant reduce the vif, drop the feature. Multicollinearity in real world can never be 0. You need to reduce it as much possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd31647b-8ea3-4574-bb89-08cc78e858ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 7\n",
    "A polynomial regression model is a machine learning model that can capture non-linear relationships between variables by fitting a non-linear regression line, which may not be possible with simple linear regression. It is used when linear regression models may not adequately capture the complexity of the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e198bf49-dcd0-450c-9069-baaf7cc86d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 8\n",
    "\n",
    "Linear Regression - \n",
    "#Advatange - \n",
    "(1) Work well irrespective of the datasize \n",
    "(2) Give information about the relevence of feature\n",
    "# Disadvanatge-\n",
    "(1) The Assumption of linear regression \n",
    "\n",
    "Polynomial Regression -\n",
    "# Advantage - \n",
    "(1) Work on any size of the dataset\n",
    "(2) Work very well on non-linear regression\n",
    "# Disadvantage -\n",
    "(1) We need to choose the right polynomial degree for good bias/ variance trade off \n",
    "\n",
    "\n",
    "Polynomial regression is used when there is no linear correlation between the variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
